{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST\n",
    "Here we will be using keras for digit recognition. We will be using MNIST as our training data for our neural network and then we will test the data and display the results. \n",
    "\n",
    "We will be using numpy to get and store the bytes in an array then we will be using keras to train our neural network.\n",
    "\n",
    "First we take the files taken from:(http://yann.lecun.com/exdb/mnist/) using gzip which is a module that provides open(), compress() and decompress() convenience functions. The GzipFile class reads and writes gzip-format files, automatically compressing or decompressing the data so that it looks like an ordinary file object.\n",
    "\n",
    "Using numpy we convert the files to a 2D array then reshape it to 28X28 every 784 bytes after the first 16 as unsigned intergers using np.uint8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For unzipping the file within the script.\n",
    "import gzip\n",
    "with gzip.open('data/t10k-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    file_content_images = f.read()\n",
    "    \n",
    "# For unzipping the file within the script.\n",
    "with gzip.open('data/t10k-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    file_content_labels = f.read()\n",
    "\n",
    "import numpy as np\n",
    "image = ~np.array(list(file_content_images[16:800])).reshape(28,28).astype(np.uint8)\n",
    "\n",
    "with gzip.open('data/train-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    train_img = f.read()\n",
    "\n",
    "with gzip.open('data/train-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    train_lbl = f.read()\n",
    "    \n",
    "train_img = ~np.array(list(train_img[16:])).reshape(60000, 28, 28).astype(np.uint8)/255.0\n",
    "train_lbl =  np.array(list(train_lbl[ 8:])).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://www.inria.fr/var/inria/storage/images/medias/actualites/generales/images-chapo/scikit-learn-chapo/1870065-1-fre-FR/scikit-learn-chapo_vignette.png)\n",
    "Scikit-learn (formerly scikits.learn) is a free software machine learning library for the Python programming language. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For encoding categorical variables and pre processing.\n",
    "import sklearn.preprocessing as pre\n",
    "\n",
    "encoder = pre.LabelBinarizer()\n",
    "encoder.fit(train_lbl)\n",
    "outputs = encoder.transform(train_lbl)\n",
    "\n",
    "outputs[0]\n",
    "\n",
    "inputs = train_img.reshape(60000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "There are 6 hidden layers made with a dropout layer running sequentially.\n",
    "\n",
    "The model takes in the 28X28 array of 784 where each pixel is a number ranging from 0 to 1 where 0 is black and 1 is white. Each one of these pixels is considered a neuron and passed into the neural network where the greyscale value is the weight.\n",
    "\n",
    "![title](https://achintavarna.files.wordpress.com/2017/11/mnist_2layers.png)\n",
    "\n",
    "\n",
    "![title](https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png)\n",
    "\n",
    "Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niall\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# ------- MODEL -------\n",
    "# Import keras.\n",
    "import keras as kr\n",
    "# Importing the required Keras modules containing model and layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout \n",
    "\n",
    "# Start a neural network, building it by layers.\n",
    "model = kr.models.Sequential()\n",
    "# Add a hidden layer with 784 neurons.\n",
    "model.add(kr.layers.Dense(units=784, activation='relu', input_dim=784))\n",
    "# Add a hidden layer with 455 neurons.\n",
    "model.add(kr.layers.Dense(units=455, activation='relu'))\n",
    "# Add a hidden layer with 250 neurons.\n",
    "model.add(kr.layers.Dense(units=250, activation='relu'))\n",
    "# Add a hidden layer with 170 neurons.\n",
    "model.add(kr.layers.Dense(units=170, activation='softplus'))\n",
    "# Add a hidden layer with 120 neurons.\n",
    "model.add(kr.layers.Dense(units=120, activation='linear'))\n",
    "# Add a hidden layer with 50 neurons.\n",
    "model.add(kr.layers.Dense(units=50, activation='relu'))\n",
    "# Add a dropout layer every 1 in 5.\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Add a three neuron output layer.\n",
    "model.add(kr.layers.Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 784)               615440    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 455)               357175    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 250)               114000    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 170)               42670     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 120)               20520     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 50)                6050      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 1,156,365\n",
      "Trainable params: 1,156,365\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the neural network\n",
    "Here we are traing the neural network with the data set. The model.fit() function is the function that trains the network itself we are passing in the arrays and and expected outputs along with the number of epochs and batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 25s 413us/step - loss: 0.6963 - acc: 0.7718\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 23s 386us/step - loss: 0.3061 - acc: 0.9100\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 23s 385us/step - loss: 0.2313 - acc: 0.9317\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 22s 371us/step - loss: 0.1917 - acc: 0.9437\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 23s 382us/step - loss: 0.1692 - acc: 0.9499\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 24s 399us/step - loss: 0.1496 - acc: 0.9559\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 27s 445us/step - loss: 0.1327 - acc: 0.9604\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 24s 400us/step - loss: 0.1263 - acc: 0.9624\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 23s 388us/step - loss: 0.1157 - acc: 0.9654\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 23s 391us/step - loss: 0.1104 - acc: 0.96671s - loss: 0\n"
     ]
    }
   ],
   "source": [
    "# Build the graph.\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Number of Epoch is the amount of times the training set is put through the model\n",
    "# The batch size is the amount of images the models processes at one time\n",
    "model.fit(inputs, outputs, epochs=10, batch_size=100)\n",
    "\n",
    "with gzip.open('data/t10k-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    test_img = f.read()\n",
    "\n",
    "with gzip.open('data/t10k-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    test_lbl = f.read()\n",
    "    \n",
    "test_img = ~np.array(list(test_img[16:])).reshape(10000, 784).astype(np.uint8) / 255.0\n",
    "test_lbl =  np.array(list(test_lbl[ 8:])).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9679\n"
     ]
    }
   ],
   "source": [
    "# Tests prints the sum of correct predictions out of the 10,000 test images.\n",
    "print((encoder.inverse_transform(model.predict(test_img)) == test_lbl).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addtional functions \n",
    "#### Option 1 \n",
    "Allows the user to train the neural network again.\n",
    "\n",
    "#### Option 2\n",
    "Allows the user to input their own image as long as it contained within the images folder and have given the correct name of the image. The package Pillow allows us to open images within the script we first resize the image to the size of the inputs we want 28X28 (784) then convert this image to a 2D array and pass it into the the neural network. The pictures I have supplied are from the original NIST dataset.\n",
    "\n",
    "#### Option 3\n",
    "Allows the user to train the neural network with in a number of epochs and batch size supplied by the user.\n",
    "\n",
    "All of these are contained within a while loop which will loop over until 0 is entered to exit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 to retrain model: \n",
      "2 to read an image: \n",
      "3 to train the neural netork with custom epochs and batch size \n",
      "0 to Quit:2\n",
      "Enter the file name: hsf_7_00003\n",
      "Enter the label for the image uploaded: 4\n",
      "The label for this image is:  4\n",
      "Prediction:  [4]\n",
      "1 to retrain model: \n",
      "2 to read an image: \n",
      "3 to train the neural netork with custom epochs and batch size \n",
      " 0 to Quit:0\n"
     ]
    }
   ],
   "source": [
    "def retrainNN():\n",
    "    model.fit(inputs, outputs, epochs=10, batch_size=100)\n",
    "    print((encoder.inverse_transform(model.predict(test_img)) == test_lbl).sum())\n",
    "\n",
    "def trainNNCustom(numEpc, btchSz):\n",
    "    model.fit(inputs, outputs, epochs=numEpc, batch_size=btchSz)\n",
    "    \n",
    "def importImage():\n",
    "    imageName = input(\"Enter the file name: \")\n",
    "    # use the file hsf_7_00003 for 4 or hsf_4_00016 for 5\n",
    "    userInput = input(\"Enter the label for the image uploaded: \")\n",
    "    print(\"The label for this image is: \", userInput)\n",
    "    # Used to open image file using pillow\n",
    "    from PIL import Image\n",
    "    temp = Image.open(\"Images/\" + imageName + \".png\").convert('L')\n",
    "    # Convert to a 1 dimensioanl array with 784 nodes\n",
    "    temp = temp.resize((28, 28))\n",
    "    imgArray = np.array(temp)\n",
    "    # converts the array to a a 1D array of 784 nodes\n",
    "    imgArray = imgArray.reshape(1, 784)\n",
    "\n",
    "    prediction = model.predict(imgArray)\n",
    "    print(\"Prediction: \", prediction.argmax(axis=1))\n",
    "    \n",
    "option =int(input(\"1 to retrain model: \\n2 to read an image: \\n3 to train the neural netork with custom epochs and batch size \\n0 to Quit:\"))\n",
    "while option != 0:\n",
    "    if option==1:\n",
    "        retrainNN()\n",
    "        option = input(\"1 to retrain model: \\n2 to read an image: \\n3 to train the neural netork with custom epochs and batch size \\n 0 to Quit:\")\n",
    "    elif option==2:\n",
    "        importImage()\n",
    "        option = input(\"1 to retrain model: \\n2 to read an image: \\n3 to train the neural netork with custom epochs and batch size \\n 0 to Quit:\")\n",
    "    elif option==3:\n",
    "        numEpc = input(\"Enter the number of epochs: \")\n",
    "        btchSz = input(\"Enter batch size: \")\n",
    "        trainNNCustom(int(numEpc), int(btchSz))\n",
    "        option = input(\"1 to retrain model: \\n2 to read an image: \\n3 to train the neural netork with custom epochs and batch size \\n 0 to Quit:\")\n",
    "    elif option == 0:\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References \n",
    "Gzip: https://docs.python.org/3/library/gzip.html\n",
    "\n",
    "scikit-learn: https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "\n",
    "Keras: https://keras.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
